{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Multi Level Perceptron from Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we attempted to train a simple 2 layer MLP on Keras. Keras, being a high level abstracted framework, hides the details behind the model and simplifies the process. We will now try to build our own 2 layer MLP, purely out of NumPy, which will unveil the hidden components of neural network training. Similar to past from-scratch attempts, we will start by creating a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a class `MLPTwoLayers`\n",
    "\n",
    "- One of the starting points to take care of while building your network is to initialize your weight matrix correctly. Consider appropriate sizes for your input, hidden and output layers - your __init__ method should take in the params `input_size`, `hidden_size`, `output_size`. Then, using these variables, initialise the weights for the hidden layers `w1`, `w2`, `b1`, and `b2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.mlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d304b8cf152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPTwoLayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.mlp'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.mlp import MLPTwoLayers as MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(3072, 100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a `forward ` method, which takes in a set of features\n",
    "- Create the `forward` method to calculate the predicted class probabilities of an image. This is known as a forward pass.  You should wrap the hidden layer with a sigmoid function (or others if you prefer), and the output layer with a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your data preparation methods here, ensure your data is randomized\n",
    "preds = mlp.forward(X[0])\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a `loss` method, which takes in the predicted probability and actual label\n",
    "- Compute the loss function: This is a function of the actual label y and predicted label y. It captures how far off our predictions are from the actual target. The objective is to minimize this loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = mlp.loss(preds, y[0])\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a `backward` method, which takes in the loss\n",
    "- Using the backpropogation algorithm, execute the backward pass and adjust the weights and bias accordingly\n",
    "- You can use a default learning rate of 1e-3 for this exercise. If you would like do otherwise, you can try to implement it as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.backward(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial attempt at training\n",
    "test_loss = 0\n",
    "for i in range(3000, 3500):\n",
    "    test_loss += mlp.loss(mlp.forward(X[i]), y[i])\n",
    "print(test_loss / 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3000):\n",
    "    if i % 100:\n",
    "        print('Item {}'.format(i))\n",
    "    mlp.backward(mlp.loss(mlp.forward(X[i]), y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, re-test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "for i in range(3000, 3500):\n",
    "    test_loss += mlp.loss(mlp.forward(X[i]), y[i])\n",
    "print(test_loss / 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you see that your test loss has decreased after training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Convolutional Neural Network (CNN)\n",
    "Please attempt this section only after you have completed the rest!\n",
    "\n",
    "In the previous part, you implemented a multilayer perceptron network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as you move to bigger models. Ideally, you want to build networks using a more modular design so that you can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this part of exercise, you will implement a close to state-of-the-art deep learning model for CIFAR-10 with the Keras Deep Learning library. In addition to implementing convolutional networks of various depth, you will need to explore different update rules for optimization, and introduce **Dropout** as a regularizer, **Batch Normalization** and **Data Augmentation** as a tool to more efficiently optimize deep networks.\n",
    "\n",
    "We saw models performing >98% accuracy on `CIFAR-10`, while most state-of-the-art models cross the 97% boundary. In general, models beyond **95%** are fairly decent.\n",
    "\n",
    "## Reading resources\n",
    "\n",
    "[Dropout](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) is a regularization technique for overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks.\n",
    "\n",
    "[Batch Normalization](https://pdfs.semanticscholar.org/c1ba/ed41e4bc9401b1b2ec8ef55ba45543f7a1a3.pdf) is a technique to provide any layer in a neural network with inputs that are zero mean/unit variance.\n",
    "\n",
    "[Data Augmentation](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced) means increasing the number of data points. In terms of images, it may mean that increasing the number of images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from src.load_img import LoadImage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten,  MaxPooling2D, Conv2D,BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imgloader = LoadImage()\n",
    "imgloader.load_unpickledata()\n",
    "train_data,train_labels,test_data,test_labels=imgloader.getVisData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data/255\n",
    "test_data=test_data/255\n",
    "n_classes = 10\n",
    "train_labels = keras.utils.to_categorical(train_labels, n_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test= train_test_split(train_data,train_labels,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enhancing the performance of you existing model in part 2 with convolutional neural networks\n",
    "- The implementation of model should be done by using Keras (or PyTorch)\n",
    "- Train your designed model \n",
    "- Improve performance with algorithm tuning: Dropout, Batch normalization, Data augmentation and other optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Baseline CNN Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_base = Sequential()\n",
    "cnn_model_base.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(32,32,3)))\n",
    "cnn_model_base.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "cnn_model_base.add(Flatten())\n",
    "cnn_model_base.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_base.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/10\n",
      "37500/37500 [==============================] - 29s 762us/step - loss: 1.4884 - acc: 0.4705 - val_loss: 1.2789 - val_acc: 0.5487\n",
      "Epoch 2/10\n",
      "37500/37500 [==============================] - 25s 672us/step - loss: 1.1484 - acc: 0.5999 - val_loss: 1.1755 - val_acc: 0.5954\n",
      "Epoch 3/10\n",
      "37500/37500 [==============================] - 26s 681us/step - loss: 0.9790 - acc: 0.6620 - val_loss: 1.0678 - val_acc: 0.6258\n",
      "Epoch 4/10\n",
      "37500/37500 [==============================] - 26s 693us/step - loss: 0.8456 - acc: 0.7064 - val_loss: 1.1016 - val_acc: 0.6255\n",
      "Epoch 5/10\n",
      "37500/37500 [==============================] - 26s 690us/step - loss: 0.7279 - acc: 0.7484 - val_loss: 1.1282 - val_acc: 0.6257\n",
      "Epoch 6/10\n",
      "37500/37500 [==============================] - 26s 688us/step - loss: 0.6186 - acc: 0.7855 - val_loss: 1.2033 - val_acc: 0.6186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6e55206a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "cnn_model_base.fit(X_train, y_train,  epochs=10, verbose=1,validation_data=(X_test,y_test), callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN Model with Dropout__\n",
    "- Will randomly drop neurons in every pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_dropout = Sequential()\n",
    "cnn_model_dropout.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(32,32,3)))\n",
    "cnn_model_dropout.add(Dropout(0.25))\n",
    "cnn_model_dropout.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "cnn_model_dropout.add(Dropout(0.25))\n",
    "\n",
    "cnn_model_dropout.add(Flatten())\n",
    "cnn_model_dropout.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_dropout.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/10\n",
      "37500/37500 [==============================] - 38s 1ms/step - loss: 1.5395 - acc: 0.4483 - val_loss: 1.2618 - val_acc: 0.5555\n",
      "Epoch 2/10\n",
      "37500/37500 [==============================] - 38s 1ms/step - loss: 1.1908 - acc: 0.5807 - val_loss: 1.1530 - val_acc: 0.6007\n",
      "Epoch 3/10\n",
      "37500/37500 [==============================] - 38s 1ms/step - loss: 1.0555 - acc: 0.6312 - val_loss: 1.1240 - val_acc: 0.6058\n",
      "Epoch 4/10\n",
      "37500/37500 [==============================] - 37s 995us/step - loss: 0.9427 - acc: 0.6717 - val_loss: 1.1036 - val_acc: 0.6175\n",
      "Epoch 5/10\n",
      "37500/37500 [==============================] - 38s 1ms/step - loss: 0.8683 - acc: 0.6952 - val_loss: 1.0716 - val_acc: 0.6302\n",
      "Epoch 6/10\n",
      "37500/37500 [==============================] - 40s 1ms/step - loss: 0.8056 - acc: 0.7189 - val_loss: 1.1010 - val_acc: 0.6299\n",
      "Epoch 7/10\n",
      "37500/37500 [==============================] - 40s 1ms/step - loss: 0.7420 - acc: 0.7400 - val_loss: 1.1208 - val_acc: 0.6282\n",
      "Epoch 8/10\n",
      "37500/37500 [==============================] - 40s 1ms/step - loss: 0.6919 - acc: 0.7554 - val_loss: 1.1137 - val_acc: 0.6266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6e2802eb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "cnn_model_dropout.fit(X_train, y_train,  epochs=10, verbose=1,validation_data=(X_test,y_test), callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN Model with Dropout and Maxpool__\n",
    " - Will slide a 2x2 filter and take the max of the matrix\n",
    "     - An x*x img will become a x/2 * x/2 img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_dropout_maxpool = Sequential()\n",
    "cnn_model_dropout_maxpool.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(32,32,3)))\n",
    "cnn_model_dropout_maxpool.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model_dropout_maxpool.add(Dropout(0.25))\n",
    "cnn_model_dropout_maxpool.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "cnn_model_dropout_maxpool.add(Dropout(0.25))\n",
    "\n",
    "cnn_model_dropout_maxpool.add(Flatten())\n",
    "cnn_model_dropout_maxpool.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_dropout_maxpool.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/10\n",
      "37500/37500 [==============================] - 19s 511us/step - loss: 1.5773 - acc: 0.4336 - val_loss: 1.2821 - val_acc: 0.5513\n",
      "Epoch 2/10\n",
      "37500/37500 [==============================] - 19s 519us/step - loss: 1.2397 - acc: 0.5644 - val_loss: 1.1979 - val_acc: 0.5764\n",
      "Epoch 3/10\n",
      "37500/37500 [==============================] - 19s 499us/step - loss: 1.1116 - acc: 0.6091 - val_loss: 1.0900 - val_acc: 0.6175\n",
      "Epoch 4/10\n",
      "37500/37500 [==============================] - 18s 489us/step - loss: 1.0421 - acc: 0.6367 - val_loss: 1.0325 - val_acc: 0.6422\n",
      "Epoch 5/10\n",
      "37500/37500 [==============================] - 19s 499us/step - loss: 0.9909 - acc: 0.6530 - val_loss: 1.0056 - val_acc: 0.6526\n",
      "Epoch 6/10\n",
      "37500/37500 [==============================] - 19s 515us/step - loss: 0.9534 - acc: 0.6654 - val_loss: 0.9788 - val_acc: 0.6598\n",
      "Epoch 7/10\n",
      "37500/37500 [==============================] - 21s 573us/step - loss: 0.9149 - acc: 0.6825 - val_loss: 0.9773 - val_acc: 0.6603\n",
      "Epoch 8/10\n",
      "37500/37500 [==============================] - 21s 561us/step - loss: 0.8895 - acc: 0.6917 - val_loss: 0.9758 - val_acc: 0.6623\n",
      "Epoch 9/10\n",
      "37500/37500 [==============================] - 21s 563us/step - loss: 0.8614 - acc: 0.7011 - val_loss: 0.9853 - val_acc: 0.6601\n",
      "Epoch 10/10\n",
      "37500/37500 [==============================] - 21s 573us/step - loss: 0.8356 - acc: 0.7065 - val_loss: 0.9660 - val_acc: 0.6711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6e03f1a58>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "cnn_model_dropout_maxpool.fit(X_train, y_train,  epochs=10, verbose=1,validation_data=(X_test,y_test), callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN Model with Dropout/MaxPool/BatchNormalization__\n",
    "- Batch Normalization will normalize the values in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_dropout_maxpool_batchnormalization = Sequential()\n",
    "cnn_model_dropout_maxpool_batchnormalization.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(32,32,3)))\n",
    "cnn_model_dropout_maxpool_batchnormalization.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model_dropout_maxpool_batchnormalization.add(Dropout(0.25))\n",
    "cnn_model_dropout_maxpool_batchnormalization.add(BatchNormalization())\n",
    "cnn_model_dropout_maxpool_batchnormalization.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "cnn_model_dropout_maxpool_batchnormalization.add(Dropout(0.25))\n",
    "\n",
    "cnn_model_dropout_maxpool_batchnormalization.add(Flatten())\n",
    "cnn_model_dropout_maxpool_batchnormalization.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_dropout_maxpool_batchnormalization.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/10\n",
      "37500/37500 [==============================] - 23s 621us/step - loss: 1.4841 - acc: 0.4699 - val_loss: 1.2769 - val_acc: 0.5562\n",
      "Epoch 2/10\n",
      "37500/37500 [==============================] - 24s 630us/step - loss: 1.1779 - acc: 0.5837 - val_loss: 1.2012 - val_acc: 0.5820\n",
      "Epoch 3/10\n",
      "37500/37500 [==============================] - 24s 652us/step - loss: 1.0769 - acc: 0.6256 - val_loss: 1.7890 - val_acc: 0.4668\n",
      "Epoch 4/10\n",
      "37500/37500 [==============================] - 24s 638us/step - loss: 1.0137 - acc: 0.6456 - val_loss: 1.7074 - val_acc: 0.4971\n",
      "Epoch 5/10\n",
      "37500/37500 [==============================] - 24s 643us/step - loss: 0.9595 - acc: 0.6662 - val_loss: 0.9927 - val_acc: 0.6533\n",
      "Epoch 6/10\n",
      "37500/37500 [==============================] - 24s 643us/step - loss: 0.9220 - acc: 0.6787 - val_loss: 1.0643 - val_acc: 0.6297\n",
      "Epoch 7/10\n",
      "37500/37500 [==============================] - 23s 617us/step - loss: 0.8996 - acc: 0.6862 - val_loss: 1.0410 - val_acc: 0.6411\n",
      "Epoch 8/10\n",
      "37500/37500 [==============================] - 19s 496us/step - loss: 0.8743 - acc: 0.6921 - val_loss: 1.0111 - val_acc: 0.6487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6e5f33320>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "cnn_model_dropout_maxpool_batchnormalization.fit(X_train, y_train,  epochs=10, verbose=1,validation_data=(X_test,y_test), callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Augmentation__\n",
    " - The training data is very balanced, data augmentation might not make a big impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_labels,test_data,test_labels=imgloader.getVisData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(train_labels,columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Testing Accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0360902584075928\n",
      "Test accuracy: 0.6406\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model_dropout_maxpool_batchnormalization.evaluate(test_data, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
